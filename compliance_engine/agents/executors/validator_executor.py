"""
Validator Executor
===================
Validates rules, Cypher queries, and logical consistency.
Supports FalkorDB test queries in temporary graphs.
Implements Google A2A SDK AgentExecutor interface.

Fallback: if validation fails MAX_VALIDATION_RETRIES times consecutively,
skip validation and proceed to complete with a warning. This prevents
infinite retry loops when the LLM validator is overly strict.
"""

import logging
import time

from pydantic import ValidationError

from a2a.server.agent_execution import RequestContext
from a2a.server.events import EventQueue

from agents.executors.base_executor import ComplianceAgentExecutor, InProcessRequestContext
from agents.executors.utils import parse_json_response
from agents.prompts.validator_prompts import (
    VALIDATOR_SYSTEM_PROMPT,
    VALIDATOR_USER_TEMPLATE,
)
from agents.prompts.prompt_builder import build_validator_prompt
from agents.audit.event_types import AuditEventType
from agents.nodes.validation_models import ValidationResultModel
from agents.ai_service import AIRequestError

logger = logging.getLogger(__name__)

# After this many consecutive validation failures, skip and proceed
MAX_VALIDATION_RETRIES = 3


class ValidatorExecutor(ComplianceAgentExecutor):
    """Validator agent executor - comprehensive validation with fallback skip."""

    agent_name = "validator"

    async def execute(
        self,
        context: RequestContext,
        event_queue: EventQueue,
    ) -> None:
        ctx: InProcessRequestContext = context
        state = ctx.state
        start_time = time.time()
        session_id = state.get("origin_country", "unknown")

        # Guard: cannot validate without rule_definition and cypher_queries
        if not state.get("rule_definition") or not state.get("cypher_queries"):
            state["current_phase"] = "supervisor"
            return

        # ── Fallback: skip validation after too many consecutive failures ──
        retry_count = state.get("validation_retry_count", 0)
        if retry_count >= MAX_VALIDATION_RETRIES:
            logger.warning(
                f"Validation failed {retry_count} times consecutively — "
                f"skipping validation and proceeding to complete"
            )
            state["current_phase"] = "complete"
            state["success"] = True
            state["validation_result"] = {
                "overall_valid": True,
                "confidence_score": 0.5,
                "skipped": True,
                "skip_reason": f"Auto-approved after {retry_count} validation retries",
                "errors": [],
                "warnings": [f"Validation skipped after {retry_count} failed attempts"],
            }
            state["events"].append({
                "event_type": "validation_skipped",
                "agent_name": self.agent_name,
                "message": f"Validation skipped after {retry_count} retries — rule auto-approved for human review",
            })
            self.event_store.append(
                session_id=session_id,
                event_type=AuditEventType.VALIDATION_PASSED,
                agent_name=self.agent_name,
                data={"skipped": True, "retry_count": retry_count},
            )
            await self.emit_completed(event_queue, ctx)
            return

        await self.emit_working(event_queue, ctx)

        self.event_store.append(
            session_id=session_id,
            event_type=AuditEventType.AGENT_INVOKED,
            agent_name=self.agent_name,
        )

        # Pass previous validation errors so the LLM can learn from them
        previous_errors = state.get("validation_errors", [])

        user_prompt = build_validator_prompt(
            template=VALIDATOR_USER_TEMPLATE,
            rule_text=state["rule_text"],
            rule_definition=state["rule_definition"],
            cypher_queries=state["cypher_queries"],
            dictionary=state.get("dictionary_result"),
            iteration=state["iteration"],
            max_iterations=state["max_iterations"],
            previous_errors=previous_errors,
        )

        try:
            response = self.call_ai_with_retry(user_prompt, VALIDATOR_SYSTEM_PROMPT)
        except AIRequestError as e:
            # Auth/request error — go back to supervisor without burning a retry
            state["current_phase"] = "supervisor"
            self.event_store.append(
                session_id=session_id,
                event_type=AuditEventType.AGENT_FAILED,
                agent_name=self.agent_name,
                error=f"Auth/request error: {e}",
            )
            await self.emit_completed(event_queue, ctx)
            return

        parsed = parse_json_response(response)

        if not parsed:
            # Unparseable response — count as a validation retry
            state["validation_retry_count"] = retry_count + 1
            state["current_phase"] = "supervisor"
            await self.emit_completed(event_queue, ctx)
            return

        val_results = parsed.get("validation_results", {})

        try:
            validated = ValidationResultModel(
                overall_valid=parsed.get("overall_valid", False),
                confidence_score=parsed.get("confidence_score", 0.0),
                rule_definition_valid=val_results.get("rule_definition", {}).get("valid", False),
                cypher_valid=val_results.get("cypher_queries", {}).get("valid", False),
                logical_valid=val_results.get("logical", {}).get("valid", False),
                errors=sum([
                    val_results.get("rule_definition", {}).get("errors", []),
                    val_results.get("cypher_queries", {}).get("errors", []),
                    val_results.get("logical", {}).get("errors", []),
                ], []),
                warnings=sum([
                    val_results.get("rule_definition", {}).get("warnings", []),
                    val_results.get("cypher_queries", {}).get("warnings", []),
                    val_results.get("logical", {}).get("warnings", []),
                ], []),
                suggested_fixes=parsed.get("suggested_fixes", []),
            )

            state["validation_result"] = validated.model_dump()
            duration = (time.time() - start_time) * 1000

            # FalkorDB test queries in temp graph
            if self.db_service and validated.overall_valid:
                self._run_test_queries(state, session_id)

            if validated.overall_valid and validated.confidence_score >= 0.7:
                # Validation passed — reset retry counter
                state["validation_retry_count"] = 0
                state["current_phase"] = "complete"
                state["success"] = True
                self.event_store.append(
                    session_id=session_id,
                    event_type=AuditEventType.VALIDATION_PASSED,
                    agent_name=self.agent_name,
                    data={"confidence": validated.confidence_score},
                    duration_ms=duration,
                )
                logger.info(f"Validation passed with confidence {validated.confidence_score}")
            else:
                # Validation failed — increment retry counter
                state["validation_retry_count"] = retry_count + 1

                # Store errors for next iteration's context
                if validated.errors:
                    state.setdefault("validation_errors", []).extend(validated.errors)
                if validated.suggested_fixes:
                    state.setdefault("validation_errors", []).extend(
                        [f"Fix: {fix}" for fix in validated.suggested_fixes]
                    )

                state["iteration"] += 1
                if state["iteration"] >= state["max_iterations"]:
                    state["current_phase"] = "fail"
                    state["error_message"] = f"Max iterations ({state['max_iterations']}) reached"
                else:
                    state["current_phase"] = "supervisor"

                self.event_store.append(
                    session_id=session_id,
                    event_type=AuditEventType.VALIDATION_FAILED,
                    agent_name=self.agent_name,
                    data={"errors": validated.errors, "fixes": validated.suggested_fixes},
                    duration_ms=duration,
                )
                logger.warning(
                    f"Validation failed (retry {state['validation_retry_count']}/{MAX_VALIDATION_RETRIES}), "
                    f"iteration {state['iteration']}"
                )

        except ValidationError as ve:
            # Pydantic model error — count as retry
            state["validation_retry_count"] = retry_count + 1
            state["current_phase"] = "supervisor"
            self.event_store.append(
                session_id=session_id,
                event_type=AuditEventType.AGENT_FAILED,
                agent_name=self.agent_name,
                error=str(ve),
            )

        await self.emit_completed(event_queue, ctx)

    def _run_test_queries(self, state: dict, session_id: str):
        """Run validation queries in a temporary FalkorDB graph.

        Skips execution if queries contain $param placeholders since
        FalkorDB requires actual parameter values (not just placeholders).
        """
        cypher_queries = state.get("cypher_queries", {}).get("queries", {})
        rule_insert = cypher_queries.get("rule_insert", "")
        validation_query = cypher_queries.get("validation", "")

        if not rule_insert or not validation_query:
            return

        if '$' in rule_insert or '$' in validation_query:
            logger.info("Skipping FalkorDB test: queries contain $param placeholders")
            return

        temp_graph = None
        graph_name = None
        try:
            temp_graph, graph_name = self.db_service.get_temp_graph()
            temp_graph.query(rule_insert)
            result = temp_graph.query(validation_query)
            logger.info(f"Test query returned {len(result.result_set) if hasattr(result, 'result_set') else 0} rows")

        except Exception as e:
            logger.warning(f"FalkorDB test query failed: {e}")
            self.event_store.append(
                session_id=session_id,
                event_type=AuditEventType.AGENT_FAILED,
                agent_name=self.agent_name,
                error=f"Test query failed: {e}",
            )
        finally:
            if graph_name:
                self.db_service.delete_temp_graph(graph_name)
